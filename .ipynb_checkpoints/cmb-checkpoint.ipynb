{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating realizations of a 2d Gaussian random field $\\phi(x)$, with some simple analysis run on the position space results.  Inspired by the [Cosmic Microwave Background (CMB)](https://en.wikipedia.org/wiki/Cosmic_microwave_background); see [my website](https://garrettgoon.com/gaussian-fields/) for more background information.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Extend to higher dimensions\n",
    "- Some systematic errors seem to still exist w/ theory doing increasingly poorly and over-predicting correlations as `power` increases towards 2.  Need to find/understand, test more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import scipy.stats as stats\n",
    "import corner\n",
    "import time\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for generating the power spectrum.  Spectra are characterized by an amplitude and power-law exponent: $\\langle \\phi(k)\\phi(-k)\\rangle'=\\frac{\\texttt{amplitude}}{k^{\\texttt{power}}}\\equiv P(k)$, corresponding in position space to $\\langle \\phi(x)\\phi(0)\\rangle\\propto \\frac{\\texttt{amplitude}}{x^{d-{\\texttt{power}}}}\\equiv G(x)$ in $d$-dimensions. \n",
    "\n",
    "Important parameters:\n",
    "\n",
    "- `amplitude`: amplitude of spectrum, defined above\n",
    "- `power`: power-law exponent, defined above\n",
    "- `dimensions`: number of spatial dimensions (can only use `dimensions`=2, currently)\n",
    "- `size_exponent`: grid size is 2**`size_exponent`\n",
    "- `maxlen`: max separation of data points collected when forming binned correlation function\n",
    "    \n",
    "Methods: \n",
    "\n",
    "- `generate`: generate realization of desired power spectrum, perform simple fit as sanity check\n",
    "- `spectrum_plot`: visualization of generated spectra\n",
    "- `hist`: histogram of generated values (should be normally distributed; mostly a sanity check)\n",
    "- `bin_pair_data`: pair data separated by less than distance `maxlen` and bin to find correlation function. Produce some summary plots, perform simple linear-regression, and compare fit and theory to results.\n",
    "- `bayes`: simple bayesian/MCMC analysis on `amplitude` and `power` parameters using flat priors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "variables": {
     "fit": "<p><strong>NameError</strong>: name &#39;fit&#39; is not defined</p>\n",
     "theory": "<p><strong>NameError</strong>: name &#39;theory&#39; is not defined</p>\n",
     "{-1*correlator_lr.slope:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-3deb6ef8878c>, line 1)</p>\n",
     "{-1*spectrum_lr.slope:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-6225dfcb6557>, line 1)</p>\n",
     "{-self.power:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-edc6f4870956>, line 1)</p>\n",
     "{self.power-self.dimensions:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-1482837c0be5>, line 1)</p>\n"
    }
   },
   "outputs": [],
   "source": [
    "class CmbPy():\n",
    "    def __init__(self, amplitude=1, power=1.75, size_exponent=3, dimensions=2):\n",
    "        self.amplitude = amplitude\n",
    "        self.power = power\n",
    "        self.size = int(2**(size_exponent))\n",
    "        self.dimensions = dimensions\n",
    "        # writing P(k)=amplitude/k^{power}, then corresponding position-space correlator is\n",
    "        # G(x)=x_amplitude/x^{dims-power} with x_amplitude as below:\n",
    "        self.x_amplitude = 1 / (\n",
    "            np.pi * 2**\n",
    "            self.power) * self.amplitude * gamma(1 - (self.power / 2)) / gamma(\n",
    "                self.power / 2) * (2 * np.pi / self.size)**(self.power)\n",
    "        # the final (2*np.pi/self.size)**(self.power) factor comes from being careful in passing from continuous to discrete\n",
    "\n",
    "    def _theory_xspace(self, theta, x):\n",
    "        amplitude, power = theta\n",
    "        \"\"\"\n",
    "            The theoretical position-space correlator, given the model parameters. \n",
    "            Only valid for 2d right now. Unambiguous for 2>power>.5\n",
    "            \"\"\"\n",
    "        return 1 / (np.pi * 2**power) * amplitude * x**(\n",
    "            power - 2) * gamma(1 - (power / 2)) / gamma(\n",
    "                power / 2) * (2 * np.pi / self.size)**(self.power)\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        generating the spectrum\n",
    "        \"\"\"\n",
    "        # creating a grid w/ each point pulled from std normal\n",
    "        gaussian_seed = np.random.normal(\n",
    "            size=[self.size for _ in range(self.dimensions)])\n",
    "        # fourier transform\n",
    "        gaussian_seed_fourier = np.fft.fft2(gaussian_seed)\n",
    "        # numpy's fft algorithm automatically indexes with negative values on right half\n",
    "        # positive on left half, as desired\n",
    "\n",
    "        # relevant momenta vectors with desired fft indexing\n",
    "        # plus the related meshgrid and array of momentum norms\n",
    "        # we will include 2\\pi factors in the power spectrum\n",
    "        kvector = np.fft.fftfreq(self.size) * self.size\n",
    "        kgrid = np.meshgrid(kvector, kvector)\n",
    "        knorms = np.sqrt(kgrid[0]**2 + kgrid[1]**2)\n",
    "\n",
    "        # create the desired power spectrum with the k=0 divergence regulated to zero\n",
    "        if self.power > 0:\n",
    "            knorms[0][0] = np.inf\n",
    "        power_spectrum = self.amplitude * (1 / knorms**self.power)\n",
    "\n",
    "        # and its square root\n",
    "        power_spectrum_sqrt = np.sqrt(power_spectrum)\n",
    "        # multiply by the transformed white noise to get the realization of the spectrum\n",
    "        fourier_spectrum = gaussian_seed_fourier * power_spectrum_sqrt\n",
    "\n",
    "        # create the power spectrum\n",
    "        # https://bertvandenbroucke.netlify.app/2019/05/24/computing-a-power-spectrum-in-python/ is useful resource for this\n",
    "        # create measured power spectrum (recalling to divide by appropriate N's)\n",
    "        fourier_amplitudes = np.abs(fourier_spectrum)**2 / (self.size)**2\n",
    "        # flatten out and bin\n",
    "        fourier_amplitudes_flat = fourier_amplitudes.flatten()\n",
    "        knorms_flat = knorms.flatten()\n",
    "        kbins = np.arange(.5, self.size // 2 + 1, 1)\n",
    "        kvals = 0.5 * (kbins[1:] + kbins[:-1])\n",
    "        binned_means, _, _ = stats.binned_statistic(knorms_flat,\n",
    "                                                    fourier_amplitudes_flat,\n",
    "                                                    statistic='mean',\n",
    "                                                    bins=kbins)\n",
    "\n",
    "        # simple linear regression (treating all errors as equal)\n",
    "        # use abs on binned_means to force positive; a fudge for very low statistics data and testing\n",
    "        spectrum_lr = stats.linregress(np.log(1 / kvals),\n",
    "                                       np.log(np.abs(binned_means)))\n",
    "\n",
    "        def spectrum_lr_fit(k):\n",
    "            return np.exp(spectrum_lr.intercept) / (k**(spectrum_lr.slope))\n",
    "\n",
    "        # plotting\n",
    "        fig, axs = plt.subplots(2)\n",
    "        x_fit = np.linspace(kvals[0], kvals[-1], num=100)\n",
    "        y_fit = spectrum_lr_fit(x_fit)\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_fit,\n",
    "                      linestyle='-',\n",
    "                      color='r',\n",
    "                      label='linear-regression')\n",
    "        axs[0].plot(kvals, binned_means)\n",
    "        axs[0].set_xlabel('$k$')\n",
    "        axs[0].set_ylabel('$P(k)$')\n",
    "        axs[1].loglog(kvals, binned_means)\n",
    "        axs[1].set_xlabel('$k$')\n",
    "        axs[1].set_ylabel('$P(k)$')\n",
    "        axs[1].set_title(\n",
    "            f'$P^{{fit}}(k)\\\\approx  {np.exp(spectrum_lr.intercept):.5f}\\\\cdot k^{{{-1*spectrum_lr.slope:.5f}}} \\\\quad P^{{theory}}(k)\\\\approx  {self.amplitude:.5f}\\\\cdot k^{{{-self.power:.5f}}} $'\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power})')\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # transform back and take the real part to get the spectrum\n",
    "        self.spectrum = np.real(np.fft.ifft2(fourier_spectrum))\n",
    "\n",
    "        # imaginary parts are from numerical errors; they're very small\n",
    "        im_to_re_ratio = np.imag(np.fft.ifft2(fourier_spectrum)) / np.real(\n",
    "            np.fft.ifft2(fourier_spectrum))\n",
    "\n",
    "        print(\n",
    "            'Sanity check: ratio of imaginary to real components in generated data:'\n",
    "        )\n",
    "        print(\n",
    "            f'Average ratio: {np.mean(im_to_re_ratio)} Standard dev.: {np.std(im_to_re_ratio)}'\n",
    "        )\n",
    "\n",
    "    def spectrum_plot(self):\n",
    "        \"\"\"\n",
    "        plotting the spectrum\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'spectrum'):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(self.spectrum)\n",
    "            fig.suptitle('Realization')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Run generate to create spectrum first')\n",
    "\n",
    "    def hist(self):\n",
    "        \"\"\"\n",
    "        histogram of generated values\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'spectrum'):\n",
    "            return print('Run generate to create spectrum first')\n",
    "\n",
    "        data = self.spectrum.flatten()\n",
    "        std = np.std(data)\n",
    "        self.std = std\n",
    "        # plot data\n",
    "        _, ax = plt.subplots()\n",
    "        ax.hist(data, bins=100, density=True)\n",
    "        ax.set_ylabel('counts')\n",
    "        ax.set_xlabel('$\\phi$')\n",
    "        ax.set_title('Distribution of generated points')\n",
    "        # plot fit\n",
    "        x = np.linspace(-5 * std, 5 * std, 100)\n",
    "        y = stats.norm.pdf(x, 0, std)\n",
    "        ax.plot(x, y, label=f'Normal(0,{std**2:.5f})')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def bin_pair_data(self, maxlen=None):\n",
    "        \"\"\"\n",
    "        Collect all independent pairs of data points separated by less than maxlen\n",
    "        and bin them by distance, rounding to nearest integer. \n",
    "        Plot various aspects of data\n",
    "        \"\"\"\n",
    "        # dynamically choosing maxlen, if not specified:\n",
    "        if maxlen == None:\n",
    "            maxlen = int(self.size / 10)\n",
    "\n",
    "        print('Collecting pair data...')\n",
    "\n",
    "        #put data in array with (dist, phi1*phi2) as entries\n",
    "        pairs = []\n",
    "        # bottleneck, use progress counter, timer\n",
    "        start = time.time()\n",
    "        prog = 20\n",
    "        for i in range(self.size):\n",
    "            if (i + 1) % int(self.size / 5) == 0:\n",
    "                print(f'{prog}% complete')\n",
    "                prog += 20\n",
    "            for j in range(self.size):\n",
    "                for a in range(i + 1, min(self.size, i + maxlen)):\n",
    "                    for b in range(j + 1, min(self.size, j + maxlen)):\n",
    "                        dist = np.sqrt((i - a)**2 + (j - b)**2)\n",
    "                        pairs.append(\n",
    "                            (dist, self.spectrum[i][j] * self.spectrum[a][b]))\n",
    "        print('Data collection time:', f'{time.time()-start:.5f}', 'seconds')\n",
    "        # compute how many data pairs were analyzed:\n",
    "        print('Independent data pairs analyzed:', len(pairs))\n",
    "\n",
    "        pairs_x = [item[0] for item in pairs]\n",
    "        pairs_y = [item[1] for item in pairs]\n",
    "        xbins = np.arange(.5, maxlen, 1)\n",
    "        xvals = 0.5 * (xbins[1:] + xbins[:-1])\n",
    "        binned_corr, _, _ = stats.binned_statistic(pairs_x,\n",
    "                                                   pairs_y,\n",
    "                                                   statistic='mean',\n",
    "                                                   bins=xbins)\n",
    "        binned_corr_std, _, _ = stats.binned_statistic(pairs_x,\n",
    "                                                       pairs_y,\n",
    "                                                       statistic='std',\n",
    "                                                       bins=xbins)\n",
    "        binned_corr_count, _, _ = stats.binned_statistic(pairs_x,\n",
    "                                                         pairs_y,\n",
    "                                                         statistic='count',\n",
    "                                                         bins=xbins)\n",
    "\n",
    "        # histograms of counts and stds\n",
    "        fig, axs = plt.subplots(2)\n",
    "        axs[0].bar(xvals, binned_corr_count)\n",
    "        axs[0].set_xlabel('x')\n",
    "        axs[0].set_title('Points per bin')\n",
    "        axs[1].scatter(xvals, binned_corr_std)\n",
    "        axs[1].set_xlabel('x')\n",
    "        axs[1].set_ylabel('$\\\\sigma$')\n",
    "        axs[1].set_title('Std. of $\\\\phi(x)\\\\phi(0)$ values per bin')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # simple linear regression (treating all errors as equal)\n",
    "        # use abs on binned_corr to force positive; a fudge for very low statistics data and testing\n",
    "        correlator_lr = stats.linregress(np.log(1 / xvals),\n",
    "                                         np.log(np.abs(binned_corr)))\n",
    "        # save\n",
    "        self.lr_fit = {\n",
    "            'x_amplitude': np.exp(correlator_lr.intercept),\n",
    "            'power': self.dimensions - correlator_lr.slope\n",
    "        }\n",
    "\n",
    "        # fit fn\n",
    "        def correlator_lr_fit(x):\n",
    "            return np.exp(correlator_lr.intercept) / (x**(correlator_lr.slope))\n",
    "\n",
    "        # plotting\n",
    "        fig, axs = plt.subplots(2)\n",
    "        # data\n",
    "        axs[0].errorbar(xvals,\n",
    "                        binned_corr,\n",
    "                        yerr=binned_corr_std / np.sqrt(binned_corr_count),\n",
    "                        marker='.',\n",
    "                        color='k')\n",
    "        axs[1].errorbar(xvals,\n",
    "                        binned_corr,\n",
    "                        yerr=binned_corr_std / np.sqrt(binned_corr_count),\n",
    "                        marker='.',\n",
    "                        color='k')\n",
    "        # fit\n",
    "        x_fit = np.linspace(xvals[0], xvals[-1], num=100)\n",
    "        y_fit = correlator_lr_fit(x_fit)\n",
    "        axs[0].plot(x_fit,\n",
    "                    y_fit,\n",
    "                    linestyle='-',\n",
    "                    color='r',\n",
    "                    label='linear-regression')\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_fit,\n",
    "                      linestyle='-',\n",
    "                      color='r',\n",
    "                      label='linear-regression')\n",
    "        axs[0].plot(xvals, binned_corr)\n",
    "        axs[0].set_xlabel('$x$')\n",
    "        axs[0].set_ylabel('$\\\\langle \\\\phi(x)\\\\phi(0)\\\\rangle\\equiv G(x)$')\n",
    "        axs[1].loglog(xvals, binned_corr)\n",
    "        axs[1].set_xlabel('$x$')\n",
    "        axs[1].set_ylabel('$\\\\langle \\\\phi(x)\\\\phi(0)\\\\rangle\\equiv G(x)$')\n",
    "        # theory\n",
    "        # plotting theory prediction:\n",
    "        y_theory = self._theory_xspace((self.amplitude, self.power), x_fit)\n",
    "        axs[0].plot(x_fit, y_theory, linestyle=':', color='m', label='theory')\n",
    "        axs[0].plot(x_fit, y_theory, linestyle=':', color='m', label='theory')\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_theory,\n",
    "                      linestyle=':',\n",
    "                      color='m',\n",
    "                      label='theory')\n",
    "\n",
    "        x_amp = 1 / (np.pi * 2**self.power) * self.amplitude * gamma(\n",
    "            1 - (self.power / 2)) / gamma(\n",
    "                self.power / 2) * (2 * np.pi / self.size)**(self.power)\n",
    "        axs[1].set_title(\n",
    "            f'$G^{{fit}}(x)\\\\approx  {np.exp(correlator_lr.intercept):.5f}\\\\cdot x^{{{-1*correlator_lr.slope:.5f}}} \\\\quad G^{{theory}}(x)\\\\approx  {x_amp:.5f}\\\\cdot x^{{{self.power-self.dimensions:.5f}}} $'\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power:.5f})'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # compute summary statistics: (x,y_bar,std(y_bar))\n",
    "        data_summary = np.array([[\n",
    "            xvals[i], binned_corr[i],\n",
    "            binned_corr_std[i] / np.sqrt(binned_corr_count[i])\n",
    "        ] for i in range(len(xvals)) if binned_corr_count[i] > 0])\n",
    "\n",
    "        self.data_summary = data_summary\n",
    "\n",
    "    def bayes(self, steps=10**4, walkers=2**6):\n",
    "        \"\"\"\n",
    "        bayesian analysis for power-spectrum parameters using binned data\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'data_summary'):\n",
    "            return print('Run data_fit first')\n",
    "\n",
    "        # set flat priors on the amplitude and power over some range covering actual values\n",
    "        amp_max, power_max = 3 * self.amplitude, 10\n",
    "\n",
    "        def log_prior(theta):\n",
    "            amplitude, power = theta\n",
    "            if 0 < amplitude < amp_max and 0 < power < power_max:\n",
    "                return 0.0\n",
    "            return -np.inf\n",
    "\n",
    "        def log_likelihood(theta, data):\n",
    "            x, y, sigy = data\n",
    "            return np.sum(-np.log(sigy) - .5 *\n",
    "                          (self._theory_xspace(theta, x) - y)**2 / sigy**2)\n",
    "\n",
    "        # total log-prob needed for MCMC\n",
    "        def log_posterior(theta, data):\n",
    "            lp = log_prior(theta)\n",
    "            if not np.isfinite(lp):\n",
    "                return -np.inf\n",
    "            return lp + log_likelihood(theta, data)\n",
    "\n",
    "        # MCMC setup\n",
    "        # distribute initial walker positions around position space LR best fits (so as not to cheat)\n",
    "        initial = np.array([self.lr_fit['x_amplitude'], self.lr_fit['power']])\n",
    "        pos = initial.T + .1 * np.concatenate(\n",
    "            (np.random.uniform(-initial[0], initial[0], (walkers, 1)),\n",
    "             np.random.uniform(-initial[1], initial[1], (walkers, 1))),\n",
    "            axis=1)\n",
    "        walkers, dim = pos.shape\n",
    "\n",
    "        sampler = emcee.EnsembleSampler(walkers,\n",
    "                                        dim,\n",
    "                                        log_posterior,\n",
    "                                        args=[self.data_summary.T])\n",
    "        sampler.run_mcmc(pos, steps, progress=True)\n",
    "        samples = sampler.get_chain()\n",
    "\n",
    "        # plotting chain convergence\n",
    "        fig, axs = plt.subplots(2)\n",
    "        for i in range(walkers):\n",
    "            for j in range(dim):\n",
    "                axs[j].plot(samples[:, i, j])\n",
    "\n",
    "        axs[0].set_ylabel('amp')\n",
    "        axs[1].set_ylabel('power')\n",
    "        fig.suptitle('Chain convergence')\n",
    "        plt.show()\n",
    "\n",
    "        # autocorrelation analysis\n",
    "        auto_corr = sampler.get_autocorr_time()\n",
    "        thin_rate = int(np.mean(np.array(auto_corr)) / 2)\n",
    "\n",
    "        # burn 1/4 of data, then make corner plots\n",
    "        flat_samples = sampler.get_chain(discard=int(steps / 4),\n",
    "                                         thin=thin_rate,\n",
    "                                         flat=True)\n",
    "        amp_samples, power_samples = flat_samples[:, 0], flat_samples[:, 1]\n",
    "\n",
    "        fig = corner.corner(flat_samples,\n",
    "                            labels=['amp', 'power'],\n",
    "                            truths=[self.amplitude, self.power],\n",
    "                            truth_color='r')\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power:.5f})'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a realization and analyze.  Choose the amplitude and power randomly.\n",
    "\n",
    "The momentum space correlator $P(k)=\\frac{\\texttt{amplitude}}{k^{\\texttt{power}}}$ only unambiguously defines a position-space correlator $G(x)$ for $2>{\\texttt{power}}>1/2$ (other values require regularization/analytic continuation) and so we restrict to this range ([relevant integral can be found here](https://dlmf.nist.gov/10.22#E43)).\n",
    "\n",
    "Some parameter choices which run in reasonable time: `size_exponent`=8, `maxlen`=15, `steps`=5000, `walkers`=64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amplitude: 5.89284 \n",
      "power: 1.93040\n"
     ]
    }
   ],
   "source": [
    "rand_amp, rand_power = np.random.uniform(0, 10,\n",
    "                                         1)[0], np.random.uniform(.51, 1.99,\n",
    "                                                                  1)[0]\n",
    "print(f'amplitude: {rand_amp:.5f}', f'\\npower: {rand_power:.5f}')\n",
    "\n",
    "c = CmbPy(size_exponent=8, amplitude=rand_amp, power=rand_power)\n",
    "c.generate()\n",
    "c.spectrum_plot()\n",
    "c.hist()\n",
    "c.bin_pair_data(maxlen=15)\n",
    "c.bayes(steps=5*10**3, walkers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit7f73d2527e45441faf021e5957178292"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
