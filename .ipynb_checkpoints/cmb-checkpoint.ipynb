{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating realizations of a 2d Gaussian random field $\\phi(x)$, with some simple analysis run on the position space results.  Inspired by the [Cosmic Microwave Background (CMB)](https://en.wikipedia.org/wiki/Cosmic_microwave_background); see [my website](https://garrettgoon.com/gaussian-fields/) for more background information.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Extend to higher dimensions\n",
    "- Some systematic errors seem to still exist w/ theory doing increasingly poorly and over-predicting correlations as `power` increases towards 2.  Need to find/understand, test more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import scipy.stats as stats\n",
    "import corner\n",
    "import time\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for generating the power spectrum.  Spectra are characterized by an amplitude and power-law exponent: $\\langle \\phi(k)\\phi(-k)\\rangle'=\\frac{\\texttt{amplitude}}{k^{\\texttt{power}}}\\equiv P(k)$, corresponding in position space to $\\langle \\phi(x)\\phi(0)\\rangle\\propto \\frac{\\texttt{amplitude}}{x^{d-{\\texttt{power}}}}\\equiv G(x)$ in $d$-dimensions. \n",
    "\n",
    "Important parameters:\n",
    "\n",
    "- `amplitude`: amplitude of spectrum, defined above\n",
    "- `power`: power-law exponent, defined above\n",
    "- `dimensions`: number of spatial dimensions (can only use `dimensions`=2, currently)\n",
    "- `size_exponent`: grid size is 2**`size_exponent`\n",
    "- `max_len`: maximum separation of data points collected when forming binned correlation function\n",
    "- `min_len`: minimum separation of data points collected when forming binned correlation function\n",
    "    \n",
    "Methods: \n",
    "\n",
    "- `generate`: generate realization of desired power spectrum, perform simple fit as sanity check\n",
    "- `spectrum_plot`: visualization of generated spectra\n",
    "- `hist`: histogram of generated values (should be normally distributed; mostly a sanity check)\n",
    "- `bin_pair_data`: pair data separated by less than distance `max_len` and bin to find correlation function. Produce some summary plots, perform simple linear-regression, and compare fit and theory to results.\n",
    "- `bayes`: simple bayesian/MCMC analysis on `amplitude` and `power` parameters using flat priors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "variables": {
     "fit": "<p><strong>NameError</strong>: name &#39;fit&#39; is not defined</p>\n",
     "theory": "<p><strong>NameError</strong>: name &#39;theory&#39; is not defined</p>\n",
     "{-1*correlator_lr.slope:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-3deb6ef8878c>, line 1)</p>\n",
     "{-1*spectrum_lr.slope:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-6225dfcb6557>, line 1)</p>\n",
     "{-self.power:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-edc6f4870956>, line 1)</p>\n",
     "{self.power-self.dimensions:.5f": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-2-1482837c0be5>, line 1)</p>\n"
    }
   },
   "outputs": [],
   "source": [
    "class CmbPy():\n",
    "    \"\"\"\n",
    "    A class for generating 2D power specta and some analysis of the results.\n",
    "    \"\"\"\n",
    "    def __init__(self, amplitude=1, power=1.75, size_exponent=5, dimensions=2):\n",
    "        self.amplitude = amplitude\n",
    "        self.power = power\n",
    "        self._size = int(2 ** size_exponent)\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        \"\"\"\n",
    "        Writing P(k)=amplitude/k^{power}, then corresponding position-space correlator is\n",
    "        G(x)=_x_amplitude/x^{dims-power} with _x_amplitude as below.\n",
    "        \"\"\"\n",
    "        self._x_amplitude_numerator = self.amplitude * gamma(1 - (self.power / 2))\n",
    "        self._x_amplitude_denominator = (np.pi * (2 ** self.power)) * gamma(self.power / 2)\n",
    "        self._x_amplitude = self._x_amplitude_numerator / self._x_amplitude_denominator\n",
    "        \n",
    "\n",
    "    def _theory_xspace(self, params, x):\n",
    "        amplitude, power = params\n",
    "        \"\"\"\n",
    "        The theoretical position-space correlator, given the model parameters. \n",
    "        Only valid for 2d right now. Fourier transform to position space is\n",
    "        unambiguous for 2 > power > .5. Used for fitting. Might be missing some\n",
    "        (2 * np.pi / self._size)  factors?\n",
    "\n",
    "        \"\"\"\n",
    "        numerator = amplitude * gamma(1 - (power / 2))\n",
    "        denominator = (np.pi * (2 ** power)) * gamma(power / 2)\n",
    "        return numerator * (x ** (power - 2)) / denominator\n",
    "\n",
    "    \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generating the spectrum.\n",
    "        \"\"\"\n",
    "        # Creating a grid w/ each point pulled from std normal.\n",
    "        gaussian_seed = np.random.normal(\n",
    "            size=[self._size for _ in range(self.dimensions)])\n",
    "        gaussian_seed_fourier = np.fft.fft2(gaussian_seed)\n",
    "        # Numpy's fft algorithm automatically indexes with negative values on right half\n",
    "        # positive on left half, as desired.\n",
    "\n",
    "        # Generating the fft momenta indices and their norms.\n",
    "        kvector = np.fft.fftfreq(self._size) * self._size\n",
    "        kgrid = np.meshgrid(kvector, kvector)\n",
    "        # include 2 pi / N factors\n",
    "        knorms = np.sqrt(kgrid[0] ** 2 + kgrid[1] ** 2)\n",
    "\n",
    "        # Create the desired power spectrum with the k=0 divergence regulated to zero.\n",
    "        if self.power > 0:\n",
    "            knorms[0][0] = np.inf\n",
    "        # 2 pi / N factors included here\n",
    "        power_spectrum = self.amplitude * ((knorms * (2 * np.pi / self._size)) ** (-1 * self.power))\n",
    "        power_spectrum_sqrt = np.sqrt(power_spectrum)\n",
    "        # Multiply by the transformed white noise to get the realization of the spectrum.\n",
    "        fourier_realization = gaussian_seed_fourier * power_spectrum_sqrt\n",
    "\n",
    "        # Create the power spectrum.\n",
    "        # https://bertvandenbroucke.netlify.app/2019/05/24/computing-a-power-spectrum-in-python/ is useful resource for this\n",
    "        # We need 1/N factors to match conventions for continuous case.\n",
    "        fourier_amplitudes = (np.abs(fourier_realization) ** 2) / ((self._size) ** self.dimensions)\n",
    "        \n",
    "        # Flatten out and bin.\n",
    "        fourier_amplitudes_flat = fourier_amplitudes.flatten()\n",
    "        knorms_flat = knorms.flatten()\n",
    "        k_bins = np.arange(.5, self._size // 2 + 1, 1) \n",
    "        k_vals = 0.5 * (k_bins[1:] + k_bins[:-1])\n",
    "        binned_means, _, _ = stats.binned_statistic(knorms_flat,\n",
    "                                                    fourier_amplitudes_flat,\n",
    "                                                    statistic='mean',\n",
    "                                                    bins=k_bins)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        In order to perform a power-law linear regression fit on the binned values,\n",
    "        we need to take a logarithm which requires all binned values to be positive,\n",
    "        while some are negative at large k, presumably due to low statistics.\n",
    "        As a bit of a fudge, we isolate the terms in binned_means which are positive.\n",
    "        \"\"\"\n",
    "        \n",
    "        binned_means_first_neg_index = 0\n",
    "        for index, num in enumerate(binned_means):\n",
    "            if num < 0:\n",
    "                binned_means_first_neg_index = index\n",
    "                break\n",
    "        if binned_means_first_neg_index:\n",
    "            binned_means_positive = binned_means[:binned_means_first_neg_index] \n",
    "            k_vals_positive = k_vals[:binned_means_first_neg_index]\n",
    "        else:\n",
    "            binned_means_positive = binned_means\n",
    "            k_vals_positive = k_vals\n",
    "\n",
    "        # Simple linear regression. Need 2 pi / N factors here.\n",
    "        spectrum_lr = stats.linregress(np.log(1 / (k_vals_positive * (2 * np.pi / self._size))),\n",
    "                                       np.log(np.abs(binned_means_positive)))\n",
    "        \n",
    "        def spectrum_lr_fit(k):\n",
    "            return np.exp(spectrum_lr.intercept) / ((k * (2 * np.pi / self._size)) ** (spectrum_lr.slope))\n",
    "\n",
    "        # Plotting.\n",
    "        fig, axs = plt.subplots(2)\n",
    "        x_fit = np.linspace(k_vals_positive[0], k_vals_positive[-1], num=100)\n",
    "        y_fit = spectrum_lr_fit(x_fit)\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_fit,\n",
    "                      linestyle='-',\n",
    "                      color='r',\n",
    "                      label='linear-regression')\n",
    "        axs[0].plot(k_vals_positive, binned_means_positive)\n",
    "        axs[0].set_xlabel('$k$')\n",
    "        axs[0].set_ylabel('$P(k)$')\n",
    "        axs[1].loglog(k_vals_positive, binned_means_positive)\n",
    "        axs[1].set_xlabel('$k$')\n",
    "        axs[1].set_ylabel('$P(k)$')\n",
    "        axs[1].set_title(\n",
    "            f'$P^{{fit}}(k)\\\\approx  {np.exp(spectrum_lr.intercept):.5f}\\\\cdot k^{{{-1*spectrum_lr.slope:.5f}}} \\\\quad P^{{theory}}(k)\\\\approx  {self.amplitude:.5f}\\\\cdot k^{{{-self.power:.5f}}} $'\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power:.5f})')\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Transform back and take the real part to get the spectrum.\n",
    "        self.realization = np.real(np.fft.ifft2(fourier_realization))\n",
    "\n",
    "        # Imaginary parts are from numerical errors; they're very small.\n",
    "        im_to_re_ratio = np.imag(np.fft.ifft2(fourier_realization)) / np.real(\n",
    "            np.fft.ifft2(fourier_realization))\n",
    "\n",
    "        print(\n",
    "            'Sanity check: ratio of imaginary to real components in generated data:'\n",
    "        )\n",
    "        print(\n",
    "            f'Average ratio: {np.mean(im_to_re_ratio)} Standard dev.: {np.std(im_to_re_ratio)}'\n",
    "        )\n",
    "\n",
    "    def spectrum_plot(self):\n",
    "        \"\"\"\n",
    "        Plotting the spectrum.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'spectrum'):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(self.realization,cmap='inferno')\n",
    "            fig.suptitle('Realization')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Run generate to create spectrum first')\n",
    "\n",
    "    def hist(self):\n",
    "        \"\"\"\n",
    "        Histogram of generated values.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'spectrum'):\n",
    "            return print('Run generate to create spectrum first')\n",
    "\n",
    "        data = self.realization.flatten()\n",
    "        std = np.std(data)\n",
    "        self.std = std\n",
    "        \n",
    "        _, ax = plt.subplots()\n",
    "        ax.hist(data, bins=100, density=True)\n",
    "        ax.set_ylabel('counts')\n",
    "        ax.set_xlabel('$\\phi$')\n",
    "        ax.set_title('Distribution of generated points')\n",
    "        # plot fit\n",
    "        x = np.linspace(-5 * std, 5 * std, 100)\n",
    "        y = stats.norm.pdf(x, 0, std)\n",
    "        ax.plot(x, y, label=f'Normal(0,{std**2:.5f})')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def bin_pair_data(self, min_len=1, max_len=None):\n",
    "        \"\"\"\n",
    "        Collect all independent pairs of data points separated by less than max_len\n",
    "        and bin them by distance, rounding to nearest integer. \n",
    "        Can also specify a minimum separation using min_len, taken as 1 by default\n",
    "        \n",
    "        This is a bottleneck, use progress counter, timer. \n",
    "        Don't think there's any native way to to this in numpy.\n",
    "        The number of independent pairs for an N x N grid scales as O(N^4)\n",
    "        which makes this take forever, hence the min_len and max_len params.        \n",
    "        \n",
    "        Plot various aspects of data\n",
    "        \"\"\"\n",
    "        # Dynamically choosing max_len, if not specified.\n",
    "        if max_len == None:\n",
    "            max_len = int(self._size / 10)\n",
    "\n",
    "        print('Collecting pair data...')\n",
    "\n",
    "        # Put distance and field values in two separate arrays\n",
    "        distances, fields = [], []\n",
    "        \n",
    "        start = time.time()\n",
    "        prog = 20\n",
    "        for row_1 in range(self._size):\n",
    "            if (row_1 + 1) % int(self._size / 5) == 0:\n",
    "                print(f'{prog}% complete')\n",
    "                prog += 20\n",
    "            for col_1 in range(self._size):\n",
    "                for row_2 in range(row_1 + int(min_len / np.sqrt(2)), min(self._size, row_1 + max_len + 1)):\n",
    "                    for col_2 in range(col_1 + int(min_len / np.sqrt(2)), min(self._size, col_1 + max_len + 1)):\n",
    "                        dist = np.sqrt((row_1 - row_2) ** 2 + (col_1 - col_2) ** 2)\n",
    "                        distances.append(dist)\n",
    "                        fields.append(self.realization[row_1][col_1] * self.realization[row_2][col_2])\n",
    "        fields = np.array(fields)\n",
    "        distances = np.array(distances)\n",
    "        print('Data collection time:', f'{time.time()-start:.5f}', 'seconds')\n",
    "        print('Independent data pairs analyzed:', f'{len(distances):.3e}')\n",
    "        \n",
    "\n",
    "        x_bins = np.arange(min_len-.5, max_len+1, 1)\n",
    "        x_vals = 0.5 * (x_bins[1:] + x_bins[:-1])\n",
    "        binned_corr, _, _ = stats.binned_statistic(distances,\n",
    "                                                   fields,\n",
    "                                                   statistic='mean',\n",
    "                                                   bins=x_bins)\n",
    "        binned_corr_std, _, _ = stats.binned_statistic(distances,\n",
    "                                                       fields,\n",
    "                                                       statistic='std',\n",
    "                                                       bins=x_bins)\n",
    "        binned_corr_count, _, _ = stats.binned_statistic(distances,\n",
    "                                                         fields,\n",
    "                                                         statistic='count',\n",
    "                                                         bins=x_bins)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Histograms of counts and stds\n",
    "        fig, axs = plt.subplots(2)\n",
    "        axs[0].bar(x_vals, binned_corr_count)\n",
    "        axs[0].set_xlabel('x')\n",
    "        axs[0].set_title('Points per bin')\n",
    "        axs[1].scatter(x_vals, binned_corr_std)\n",
    "        axs[1].set_xlabel('x')\n",
    "        axs[1].set_ylabel('$\\\\sigma$')\n",
    "        axs[1].set_title('Std. of $\\\\phi(x)\\\\phi(0)$ values per bin')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \"\"\"\n",
    "        Simple linear regression.\n",
    "        \n",
    "        Some of the large x data can have negative correlations again,\n",
    "        presumably due to low statistics. For the log fit, \n",
    "        we need to throw out this part of the data to avoid\n",
    "        imaginary numbers, as before. Obviously a bit of a fudge.\n",
    "        \"\"\"\n",
    "        \n",
    "        binned_corr_first_neg_index = 0\n",
    "        for index, num in enumerate(binned_corr):\n",
    "            if num < 0:\n",
    "                binned_corr_first_neg_index = index\n",
    "                break\n",
    "        if binned_corr_first_neg_index:\n",
    "            binned_corr_positive = binned_corr[:binned_corr_first_neg_index] \n",
    "            binned_corr_std_positive = binned_corr_std[:binned_corr_first_neg_index] \n",
    "            binned_corr_count_positive = binned_corr_count[:binned_corr_first_neg_index] \n",
    "            x_vals_positive = x_vals[:binned_corr_first_neg_index] \n",
    "        else:\n",
    "            binned_corr_positive = binned_corr\n",
    "            binned_corr_std_positive = binned_corr_std\n",
    "            binned_corr_count_positive = binned_corr_count\n",
    "            x_vals_positive = x_vals\n",
    "        print(binned_corr_positive)\n",
    "\n",
    "        # Simple linear regression (treating all errors as equal).\n",
    "        correlator_lr = stats.linregress(np.log(1 / x_vals_positive),\n",
    "                                         np.log(binned_corr_positive))\n",
    "        \n",
    "        self.lr_fit = {\n",
    "            '_x_amplitude': np.exp(correlator_lr.intercept),\n",
    "            'power': self.dimensions - correlator_lr.slope\n",
    "        }\n",
    "\n",
    "    \n",
    "        def correlator_lr_fit(x):\n",
    "            return np.exp(correlator_lr.intercept) / (x**(correlator_lr.slope))\n",
    "\n",
    "        \n",
    "        fig, axs = plt.subplots(2)\n",
    "        axs[0].errorbar(x_vals_positive,\n",
    "                        binned_corr_positive,\n",
    "                        yerr=binned_corr_std_positive / np.sqrt(binned_corr_count_positive),\n",
    "                        marker='.',\n",
    "                        color='k')\n",
    "        axs[1].errorbar(x_vals_positive,\n",
    "                        binned_corr_positive,\n",
    "                        yerr=binned_corr_std_positive / np.sqrt(binned_corr_count_positive),\n",
    "                        marker='.',\n",
    "                        color='k')\n",
    "        \n",
    "        # Plotting the fit.\n",
    "        x_fit = np.linspace(x_vals_positive[0], x_vals_positive[-1], num=100)\n",
    "        y_fit = correlator_lr_fit(x_fit)\n",
    "        axs[0].plot(x_fit,\n",
    "                    y_fit,\n",
    "                    linestyle='-',\n",
    "                    color='r',\n",
    "                    label='linear-regression')\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_fit,\n",
    "                      linestyle='-',\n",
    "                      color='r',\n",
    "                      label='linear-regression')\n",
    "        axs[0].plot(x_vals_positive, binned_corr_positive)\n",
    "        axs[0].set_xlabel('$x$')\n",
    "        axs[0].set_ylabel('$\\\\langle \\\\phi(x)\\\\phi(0)\\\\rangle\\equiv G(x)$')\n",
    "        axs[1].loglog(x_vals_positive, binned_corr_positive)\n",
    "        axs[1].set_xlabel('$x$')\n",
    "        axs[1].set_ylabel('$\\\\langle \\\\phi(x)\\\\phi(0)\\\\rangle\\equiv G(x)$')\n",
    "        \n",
    "        # Plotting the theory.\n",
    "        y_theory = self._theory_xspace((self.amplitude, self.power), x_fit)\n",
    "        axs[0].plot(x_fit,\n",
    "                    y_theory,\n",
    "                    linestyle=':',\n",
    "                    color='m',\n",
    "                    label='theory')\n",
    "        axs[1].loglog(x_fit,\n",
    "                      y_theory,\n",
    "                      linestyle=':',\n",
    "                      color='m',\n",
    "                      label='theory')\n",
    "        \n",
    "        \n",
    "\n",
    "        x_amp = self._x_amplitude_numerator / self._x_amplitude_denominator\n",
    "        axs[1].set_title(\n",
    "            f'$G^{{fit}}(x)\\\\approx  {np.exp(correlator_lr.intercept):.5f}\\\\cdot x^{{{-1*correlator_lr.slope:.5f}}} \\\\quad G^{{theory}}(x)\\\\approx  {x_amp:.5f}\\\\cdot x^{{{self.power-self.dimensions:.5f}}} $'\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power:.5f})'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Compute summary statistics: (x,y_bar,std(y_bar))\n",
    "        self.data_summary_positive = np.array([[\n",
    "            x_vals_positive[i], binned_corr_positive[i],\n",
    "            binned_corr_std_positive[i] / np.sqrt(binned_corr_count_positive[i])\n",
    "        ] for i in range(len(x_vals_positive))])\n",
    "        \n",
    "\n",
    "    def bayes(self, steps=10**4, walkers=2**6):\n",
    "        \"\"\"\n",
    "        Bayesian analysis for power-spectrum parameters using binned data.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'data_summary_positive'):\n",
    "            return print('Run data_fit first')\n",
    "\n",
    "        # Set flat priors on the amplitude and power over some range covering actual values.\n",
    "        amp_max, power_max = 3 * self.amplitude, 10\n",
    "\n",
    "        def log_prior(params):\n",
    "            amplitude, power = params\n",
    "            if 0 < amplitude < amp_max and 0 < power < power_max:\n",
    "                return 0.0\n",
    "            return -np.inf\n",
    "\n",
    "        def log_likelihood(params, data):\n",
    "            x, y, sigy = data\n",
    "            return np.sum(-np.log(sigy) - .5 *\n",
    "                          (self._theory_xspace(params, x) - y)**2 / sigy**2)\n",
    "\n",
    "        # Total log-prob needed for MCMC.\n",
    "        def log_posterior(params, data):\n",
    "            lp = log_prior(params)\n",
    "            if not np.isfinite(lp):\n",
    "                return -np.inf\n",
    "            return lp + log_likelihood(params, data)\n",
    "\n",
    "        # MCMC setup.\n",
    "        # Distribute initial walker positions around position space LR best fits (so as not to cheat).\n",
    "        initial = np.array([self.lr_fit['_x_amplitude'], self.lr_fit['power']])\n",
    "        pos = initial.T + .1 * np.concatenate(\n",
    "            (np.random.uniform(-initial[0], initial[0], (walkers, 1)),\n",
    "             np.random.uniform(-initial[1], initial[1], (walkers, 1))),\n",
    "            axis=1)\n",
    "        walkers, dim = pos.shape\n",
    "\n",
    "        sampler = emcee.EnsembleSampler(walkers,\n",
    "                                        dim,\n",
    "                                        log_posterior,\n",
    "                                        args=[self.data_summary_positive.T])\n",
    "        sampler.run_mcmc(pos, steps, progress=True)\n",
    "        samples = sampler.get_chain()\n",
    "\n",
    "        # Plotting chain convergence\n",
    "        fig, axs = plt.subplots(2)\n",
    "        for i in range(walkers):\n",
    "            for j in range(dim):\n",
    "                axs[j].plot(samples[:, i, j])\n",
    "\n",
    "        axs[0].set_ylabel('amp')\n",
    "        axs[1].set_ylabel('power')\n",
    "        fig.suptitle('Chain convergence')\n",
    "        plt.show()\n",
    "\n",
    "        # Autocorrelation analysis.\n",
    "        auto_corr = sampler.get_autocorr_time()\n",
    "        thin_rate = int(np.mean(np.array(auto_corr)) / 2)\n",
    "\n",
    "        # Burn 1/4 of data, then make corner plots.\n",
    "        flat_samples = sampler.get_chain(discard=int(steps / 4),\n",
    "                                         thin=thin_rate,\n",
    "                                         flat=True)\n",
    "        amp_samples, power_samples = flat_samples[:, 0], flat_samples[:, 1]\n",
    "\n",
    "        fig = corner.corner(flat_samples,\n",
    "                            labels=['amp', 'power'],\n",
    "                            truths=[self.amplitude, self.power],\n",
    "                            truth_color='r')\n",
    "        fig.suptitle(\n",
    "            f'Actual values: (amp,power)=({self.amplitude:.5f},{self.power:.5f})'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a realization and analyze.  Choose the amplitude and power randomly.\n",
    "\n",
    "The momentum space correlator $P(k)=\\frac{\\texttt{amplitude}}{k^{\\texttt{power}}}$ only unambiguously defines a position-space correlator $G(x)$ for $2>{\\texttt{power}}>1/2$ (other values require regularization/analytic continuation; [relevant integral can be found here](https://dlmf.nist.gov/10.22#E43)).  The precise form of is $G(x)=\\frac{{\\rm amplitude}\\,\\Gamma[1-{\\rm power}/2]}{2^{\\rm power}x^{\\rm 2-power}\\,\\Gamma[{\\rm power}/2]}$.\n",
    "\n",
    "Some parameter choices which run in reasonable time: `size_exponent`=8, `max_len`=15, `steps`=5000, `walkers`=64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amplitude: 0.44866\n",
      "power: 1.74001\n",
      "size: 128\n",
      "min_len: 12\n",
      "max_len: 51\n"
     ]
    }
   ],
   "source": [
    "rand_amp= np.random.uniform(0, 10,1)[0]\n",
    "rand_power = np.random.uniform(.75, 1.75, 1)[0]\n",
    "size_exponent = 7\n",
    "# Only using a subset of the data for speed\n",
    "min_len, max_len = int(.1 * 2 ** size_exponent), int(.4 * 2 ** size_exponent)\n",
    "print(f'amplitude: {rand_amp:.5f}',\n",
    "      f'power: {rand_power:.5f}',\n",
    "      f'size: {2 ** size_exponent}',\n",
    "      f'min_len: {min_len}',\n",
    "      f'max_len: {max_len}',\n",
    "      sep='\\n')\n",
    "\n",
    "\n",
    "c = CmbPy(size_exponent=size_exponent, amplitude=rand_amp, power=rand_power)\n",
    "c.generate()\n",
    "c.spectrum_plot()\n",
    "c.hist()\n",
    "c.bin_pair_data(min_len=min_len, max_len=max_len)\n",
    "c.bayes(steps=10**4, walkers=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power 1.5 did poorly\n",
    "Power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit7f73d2527e45441faf021e5957178292"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
